{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T23:58:16.520339Z",
     "start_time": "2025-11-01T23:57:23.545991Z"
    }
   },
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 1: Installation and Basic Summarization (BART)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    \"\"\"Helper function to print Markdown in a notebook cell.\"\"\"\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 1. üì• Installation and Basic Summarization (BART) üìù\")\n",
    "printmd(\"This notebook sets up the environment and demonstrates a core task for Encoder-Decoder models: **Abstractive Summarization**.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "printmd(\"## üì• Installation (Run this cell once)\")\n",
    "printmd(\"```bash\\n!pip install transformers torch\\n```\")\n",
    "\n",
    "# Initialize the Summarization pipeline using a BART model\n",
    "try:\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=\"facebook/bart-large-cnn\",\n",
    "        tokenizer=\"facebook/bart-large-cnn\"\n",
    "    )\n",
    "    print(\"Pipeline initialized successfully with bart-large-cnn.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing pipeline (requires download): {e}\")\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=\"sshleifer/distilbart-cnn-6-6\", # Fallback to smaller model\n",
    "        tokenizer=\"sshleifer/distilbart-cnn-6-6\"\n",
    "    )\n",
    "    print(\"Falling back to distilbart-cnn-6-6.\")\n",
    "\n",
    "\n",
    "text_to_summarize = \"\"\"\n",
    "The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. \n",
    "Of the objects that orbit the Sun directly, the largest are the eight planets, with the remainder being smaller objects, \n",
    "such as the five dwarf planets and millions of small Solar System bodies. The Sun, a G-type main-sequence star, accounts \n",
    "for 99.86% of the System's known mass and is dominant by gravity.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nOriginal Text (Length: {len(text_to_summarize.split())} words):\\n> {text_to_summarize.strip()}\")\n",
    "\n",
    "# Generate the summary\n",
    "summary_result = summarizer(\n",
    "    text_to_summarize,\n",
    "    max_length=40,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")[0]['summary_text']\n",
    "\n",
    "print(f\"\\nGenerated Summary:\\n> {summary_result}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"Encoder-Decoder models, like BART, are ideal for **Seq2Seq (Sequence-to-Sequence)** tasks where the output (summary) is structurally different and shorter than the input (original text).\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin.sharma/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nitin.sharma/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 1. üì• Installation and Basic Summarization (BART) üìù"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "This notebook sets up the environment and demonstrates a core task for Encoder-Decoder models: **Abstractive Summarization**."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## üì• Installation (Run this cell once)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```bash\n!pip install transformers torch\n```"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized successfully with bart-large-cnn.\n",
      "\n",
      "Original Text (Length: 73 words):\n",
      "> The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. \n",
      "Of the objects that orbit the Sun directly, the largest are the eight planets, with the remainder being smaller objects, \n",
      "such as the five dwarf planets and millions of small Solar System bodies. The Sun, a G-type main-sequence star, accounts \n",
      "for 99.86% of the System's known mass and is dominant by gravity.\n",
      "\n",
      "Generated Summary:\n",
      "> The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. The Sun accounts for 99.86% of the System's\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Encoder-Decoder models, like BART, are ideal for **Seq2Seq (Sequence-to-Sequence)** tasks where the output (summary) is structurally different and shorter than the input (original text)."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T23:58:42.630133Z",
     "start_time": "2025-11-01T23:58:26.247962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 2: Translation (T5) - The Multi-Task Model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 2. üó£Ô∏è Translation (T5) - The Multi-Task Model üåê\")\n",
    "printmd(\"T5 (Text-to-Text Transfer Transformer) uses a unique approach: framing every task, including translation, as a **text-to-text** problem using prefixes.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "# Initialize the T5 model for translation\n",
    "try:\n",
    "    translator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"t5-small\",\n",
    "        tokenizer=\"t5-small\"\n",
    "    )\n",
    "    print(\"T5 model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing T5 pipeline: {e}\")\n",
    "\n",
    "# T5 requires a task prefix\n",
    "english_text = \"translate English to German: The cat jumped over the fence.\"\n",
    "print(f\"Input Prompt: {english_text}\")\n",
    "\n",
    "# Translate\n",
    "translation_result = translator(\n",
    "    english_text,\n",
    "    max_length=50\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(f\"\\nGenerated Translation: {translation_result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Demonstrating another T5 task (Q&A) using a different prefix\n",
    "qa_prompt = \"question: What is the largest planet in our solar system? context: Jupiter is the largest planet in our solar system, followed by Saturn.\"\n",
    "print(f\"Input Prompt: {qa_prompt}\")\n",
    "\n",
    "qa_result = translator(\n",
    "    qa_prompt,\n",
    "    max_length=20\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(f\"\\nGenerated Answer: {qa_result}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The T5 Encoder-Decoder architecture uses a **task-specific prefix** (e.g., 'translate English to German:') to condition the model. The Encoder processes the entire prefixed input, and the Decoder generates the corresponding output.\")"
   ],
   "id": "6a0cec8f6d49abed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 2. üó£Ô∏è Translation (T5) - The Multi-Task Model üåê"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "T5 (Text-to-Text Transfer Transformer) uses a unique approach: framing every task, including translation, as a **text-to-text** problem using prefixes."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 model initialized successfully.\n",
      "Input Prompt: translate English to German: The cat jumped over the fence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Translation: Die Katze sprang √ºber den Zaun.\n",
      "\n",
      "============================================================\n",
      "Input Prompt: question: What is the largest planet in our solar system? context: Jupiter is the largest planet in our solar system, followed by Saturn.\n",
      "\n",
      "Generated Answer: Jupiter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The T5 Encoder-Decoder architecture uses a **task-specific prefix** (e.g., 'translate English to German:') to condition the model. The Encoder processes the entire prefixed input, and the Decoder generates the corresponding output."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T23:59:18.818074Z",
     "start_time": "2025-11-01T23:59:16.917910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 3: Understanding Encoder-Decoder Inputs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Load T5 components explicitly\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "printmd(\"# 3. üß± Understanding Encoder-Decoder Inputs and Outputs üîç\")\n",
    "printmd(\"We look at how the input is prepared for the Encoder and what the Decoder uses for generation.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "input_text = \"summarize: The Amazon rainforest is the largest tropical rainforest in the world.\"\n",
    "target_text_prefix = \"The Amazon rainforest is very large.\"\n",
    "\n",
    "# 1. Prepare Encoder Input (Source)\n",
    "encoder_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "encoder_input_ids = encoder_input.input_ids\n",
    "encoder_attention_mask = encoder_input.attention_mask\n",
    "\n",
    "printmd(\"### Encoder Input (Source Text)\")\n",
    "print(f\"Text: '{input_text}'\")\n",
    "print(f\"Input IDs shape: {encoder_input_ids.shape}\")\n",
    "print(f\"Attention Mask shape: {encoder_attention_mask.shape}\")\n",
    "print(\"Encoder reads the entire input to create a rich context vector.\")\n",
    "\n",
    "# 2. Prepare Decoder Input (Target/Prefix)\n",
    "decoder_input = tokenizer(target_text_prefix, return_tensors=\"pt\")\n",
    "decoder_input_ids = model._shift_right(decoder_input.input_ids)\n",
    "\n",
    "printmd(\"\\n### Decoder Input (Target/Output Prefix)\")\n",
    "print(f\"Prefix Text (Training Example): '{target_text_prefix}'\")\n",
    "print(f\"Shifted Decoder Input IDs shape: {decoder_input_ids.shape}\")\n",
    "print(\"Decoder input is shifted right for causal generation, similar to GPT.\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The **Encoder receives the full, non-causal input**. The **Decoder receives a context vector** from the Encoder *plus* the generated tokens so far (or a target prefix), and generates the next token autoregressively.\")"
   ],
   "id": "7edcc7153f86ca27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 3. üß± Understanding Encoder-Decoder Inputs and Outputs üîç"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "We look at how the input is prepared for the Encoder and what the Decoder uses for generation."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Encoder Input (Source Text)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'summarize: The Amazon rainforest is the largest tropical rainforest in the world.'\n",
      "Input IDs shape: torch.Size([1, 15])\n",
      "Attention Mask shape: torch.Size([1, 15])\n",
      "Encoder reads the entire input to create a rich context vector.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n### Decoder Input (Target/Output Prefix)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix Text (Training Example): 'The Amazon rainforest is very large.'\n",
      "Shifted Decoder Input IDs shape: torch.Size([1, 8])\n",
      "Decoder input is shifted right for causal generation, similar to GPT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The **Encoder receives the full, non-causal input**. The **Decoder receives a context vector** from the Encoder *plus* the generated tokens so far (or a target prefix), and generates the next token autoregressively."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T00:02:15.495514Z",
     "start_time": "2025-11-02T00:02:12.896170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 4: Cross-Attention: The Bridge\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Load T5 components explicitly\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "printmd(\"# 4. ‚û°Ô∏è Cross-Attention: The Bridge Between Encoder and Decoder üåâ\")\n",
    "printmd(\"Cross-attention is the mechanism that allows the Decoder to condition its output generation on the **entire input sequence** processed by the Encoder.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "input_text = \"translate English to French: Today is a sunny day.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 1. Get Encoder Output\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(input_ids=input_ids)\n",
    "\n",
    "encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "printmd(\"### Encoder Output (Context Vector)\")\n",
    "print(f\"Shape of Encoder's final hidden state: {encoder_hidden_states.shape}\")\n",
    "print(\"This vector contains the 'meaning' of the input, ready for the Decoder.\")\n",
    "\n",
    "printmd(\"\\n### Cross-Attention Function (Conceptual)\")\n",
    "printmd(\"In the Decoder block, the **Query (Q)** comes from the **Decoder's self-attention output** (what it has generated so far).\")\n",
    "printmd(\"The **Key (K) and Value (V)** come from the **Encoder's final hidden state**.\")\n",
    "printmd(\"> **Cross-Attention = Attention(Q_Decoder, K_Encoder, V_Encoder)**\")\n",
    "\n",
    "printmd(\"\\nThis operation allows the Decoder to decide which parts of the input text (K/V from Encoder) are most relevant when generating the next word (Query from Decoder).\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"Unlike GPT (Decoder-only), Encoder-Decoder models use **Cross-Attention** to directly align generated output tokens with the input source tokens. This is crucial for tasks like translation and summarization.\")"
   ],
   "id": "540772353541c30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 4. ‚û°Ô∏è Cross-Attention: The Bridge Between Encoder and Decoder üåâ"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Cross-attention is the mechanism that allows the Decoder to condition its output generation on the **entire input sequence** processed by the Encoder."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Encoder Output (Context Vector)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Encoder's final hidden state: torch.Size([1, 13, 512])\n",
      "This vector contains the 'meaning' of the input, ready for the Decoder.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n### Cross-Attention Function (Conceptual)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "In the Decoder block, the **Query (Q)** comes from the **Decoder's self-attention output** (what it has generated so far)."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The **Key (K) and Value (V)** come from the **Encoder's final hidden state**."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "> **Cross-Attention = Attention(Q_Decoder, K_Encoder, V_Encoder)**"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\nThis operation allows the Decoder to decide which parts of the input text (K/V from Encoder) are most relevant when generating the next word (Query from Decoder)."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Unlike GPT (Decoder-only), Encoder-Decoder models use **Cross-Attention** to directly align generated output tokens with the input source tokens. This is crucial for tasks like translation and summarization."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 5: The Encoder's Role (Non-Causal Attention)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 5. üé≠ The Encoder's Role: Bidirectional Self-Attention (Non-Causal) üîÑ\")\n",
    "printmd(\"The Encoder processes the entire input sequence simultaneously, allowing a token to attend to all other tokens, both preceding and succeeding it.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "sequence_length = 5\n",
    "# Conceptual mask where all tokens can see each other (non-causal)\n",
    "attention_mask = torch.ones(sequence_length, sequence_length)\n",
    "\n",
    "printmd(\"### Conceptual Encoder Self-Attention Mask (Non-Causal):\")\n",
    "printmd(\"`1 = can attend, 0 = cannot attend (masked)`\")\n",
    "print(attention_mask)\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"- Row 2 (token 2): can see tokens 0, 1, 2, 3, 4.\")\n",
    "print(\"- Every token can see every other token in the input sequence (bidirectional context).\")\n",
    "\n",
    "printmd(\"\\n### Decoder Self-Attention Mask (Recap - Causal):\")\n",
    "decoder_mask = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "print(decoder_mask)\n",
    "print(\"The Decoder's self-attention *still* uses a causal mask, like GPT, to ensure it only uses previously generated tokens.\")\n",
    "\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The **Encoder** uses **bidirectional (non-causal) attention** to build a comprehensive understanding of the input. This is its key difference from the GPT-style decoder.\")"
   ],
   "id": "980616aa8ceba59e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T00:03:27.926599Z",
     "start_time": "2025-11-02T00:03:25.518582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 6: Understanding Inputs for Generation vs. Training\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "printmd(\"# 6. üìè Decoder Input: Generation vs. Training Mode üîÑ\")\n",
    "printmd(\"The way the Decoder receives its initial sequence differs significantly between training (teacher forcing) and inference (autoregression).\")\n",
    "printmd(\"---\")\n",
    "\n",
    "input_text = \"translate English to French: The clock struck midnight.\"\n",
    "\n",
    "printmd(\"### Mode 1: Training (Teacher Forcing)\")\n",
    "printmd(\"During training, the **target labels (correct translation)** are shifted right to serve as the Decoder's input sequence.\")\n",
    "\n",
    "target_text = \"L'horloge sonna minuit.\"\n",
    "decoder_input_ids_training = tokenizer(target_text, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids_training = model._shift_right(decoder_input_ids_training)\n",
    "\n",
    "print(f\"Target (Labels): {tokenizer.decode(tokenizer(target_text, return_tensors='pt').input_ids[0])}\")\n",
    "print(f\"Decoder Input (Training): {tokenizer.decode(decoder_input_ids_training[0])}\")\n",
    "printmd(\"This allows the Decoder to learn efficiently by seeing the correct answer at every step.\")\n",
    "\n",
    "printmd(\"\\n### Mode 2: Generation (Inference)\")\n",
    "printmd(\"During generation, the Decoder starts with a single **decoder_start_token** and generates tokens one by one.\")\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids, max_length=15)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Output: {generated_text}\")\n",
    "printmd(\"This is a true autoregressive process, relying on the Decoder's own output from the previous step.\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The Decoder is autoregressive in **both modes**: in training it uses the *correct* previous token, and in inference, it uses the *generated* previous token.\")"
   ],
   "id": "cd0462cdfa789e0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 6. üìè Decoder Input: Generation vs. Training Mode üîÑ"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The way the Decoder receives its initial sequence differs significantly between training (teacher forcing) and inference (autoregression)."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Mode 1: Training (Teacher Forcing)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "During training, the **target labels (correct translation)** are shifted right to serve as the Decoder's input sequence."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (Labels): L'horloge sonna minuit.</s>\n",
      "Decoder Input (Training): <pad> L'horloge sonna minuit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "This allows the Decoder to learn efficiently by seeing the correct answer at every step."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n### Mode 2: Generation (Inference)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "During generation, the Decoder starts with a single **decoder_start_token** and generates tokens one by one."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: translate English to French: The clock struck midnight.\n",
      "Generated Output: L'horloge a frapp√© minuit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "This is a true autoregressive process, relying on the Decoder's own output from the previous step."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The Decoder is autoregressive in **both modes**: in training it uses the *correct* previous token, and in inference, it uses the *generated* previous token."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 7: Conditional Generation (Summarization Parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Initialize the Summarization pipeline using a smaller BART model\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-6-6\",\n",
    "    tokenizer=\"sshleifer/distilbart-cnn-6-6\"\n",
    ")\n",
    "\n",
    "printmd(\"# 7. üß© Conditional Generation Parameters (Summarization) ‚öôÔ∏è\")\n",
    "printmd(\"Encoder-Decoder models rely heavily on generation parameters like Beam Search and length constraints to control the output sequence.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "long_article = \"\"\"\n",
    "The Hubble Space Telescope (HST) is a space telescope that was launched into low Earth orbit in 1990 and remains in operation. \n",
    "It was not the first space telescope, but it is one of the largest and most versatile. Hubble is famous for its crucial \n",
    "role as a research tool and as a public relations boost for astronomy. It is named after astronomer Edwin Hubble. \n",
    "Hubble has provided some of the most detailed visible light images ever taken, allowing a deep view into space \n",
    "and time. Hubble's successor, the James Webb Space Telescope (JWST), was launched in December 2021.\n",
    "\"\"\"\n",
    "print(f\"Original Text (approx. 85 words):\\n> {long_article.strip()}\")\n",
    "\n",
    "printmd(\"\\n### Example 1: Short Summary (Controlled Length)\")\n",
    "summary_1 = summarizer(\n",
    "    long_article,\n",
    "    max_length=20,\n",
    "    min_length=10,\n",
    "    do_sample=False,\n",
    "    num_beams=4\n",
    ")[0]['summary_text']\n",
    "print(f\"Summary (Max 20 tokens): {summary_1}\")\n",
    "\n",
    "printmd(\"\\n### Example 2: Longer Summary (Controlled Length)\")\n",
    "summary_2 = summarizer(\n",
    "    long_article,\n",
    "    max_length=50,\n",
    "    min_length=30,\n",
    "    do_sample=False,\n",
    "    num_beams=4\n",
    ")[0]['summary_text']\n",
    "print(f\"Summary (Max 50 tokens): {summary_2}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"In Encoder-Decoder tasks, parameters like **Beam Search** and **length constraints** (`max_length`, `min_length`) are essential because the output sequence length is decoupled from the input sequence length.\")"
   ],
   "id": "ee331533e06b361b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 8: Visualizing Sequence Length Change\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Initialize the Translation pipeline using a T5 model\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_de\",\n",
    "    model=\"t5-small\",\n",
    "    tokenizer=\"t5-small\"\n",
    ")\n",
    "\n",
    "printmd(\"# 8. üìä Visualizing Sequence Length Change (Encoder vs. Decoder) üìê\")\n",
    "printmd(\"Encoder-Decoder models allow the output sequence length to be different from the input sequence length (Seq2Seq).\")\n",
    "printmd(\"---\")\n",
    "\n",
    "english_phrase = \"The rapid development of large language models has transformed the landscape of artificial intelligence research.\"\n",
    "\n",
    "# 1. Tokenize Input (Encoder Side)\n",
    "input_token_ids = translator.tokenizer.encode(english_phrase, return_tensors=\"pt\")[0]\n",
    "input_length = input_token_ids.size(0)\n",
    "\n",
    "print(f\"English Input: {english_phrase}\")\n",
    "print(f\"Encoder Input Token Length: {input_length}\")\n",
    "\n",
    "# 2. Generate Output (Decoder Side)\n",
    "output = translator(english_phrase, max_length=100)\n",
    "german_translation = output[0]['translation_text']\n",
    "output_token_ids = translator.tokenizer.encode(german_translation, return_tensors=\"pt\")[0]\n",
    "output_length = output_token_ids.size(0)\n",
    "\n",
    "print(f\"\\nGerman Output: {german_translation}\")\n",
    "print(f\"Decoder Output Token Length: {output_length}\")\n",
    "\n",
    "# Conceptual Length Comparison\n",
    "printmd(\"\\n### Conceptual Length Comparison\")\n",
    "comparison = \"Output length can vary\"\n",
    "if output_length > input_length:\n",
    "    comparison = \"Output > Input (Expansion)\"\n",
    "elif output_length < input_length:\n",
    "    comparison = \"Output < Input (Compression)\"\n",
    "else:\n",
    "    comparison = \"Output ‚âà Input\"\n",
    "\n",
    "print(f\"Length Difference: {comparison}\")\n",
    "print(f\"Input Tokens:  {'‚ñà' * input_length}\")\n",
    "print(f\"Output Tokens: {'‚ñà' * output_length}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"Encoder-Decoder models excel at **Sequence Transformation** because the Decoder's length is determined only by when it predicts the **End-of-Sentence (EOS)** token.\")"
   ],
   "id": "32667a76e5c3f774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 9: Encoder-Decoder Interaction: encoder_hidden_states\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Load T5 components explicitly\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "printmd(\"# 9. üîó Encoder-Decoder Interaction: Accessing Hidden States üß†\")\n",
    "printmd(\"The Encoder's final hidden state is the consolidated **context vector** passed to the Decoder via cross-attention.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "input_text = \"translate English to French: This is a beautiful day.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 1. Forward Pass through the Encoder\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(input_ids=input_ids)\n",
    "\n",
    "# The final layer's hidden state is the context vector\n",
    "context_vector = encoder_outputs.last_hidden_state\n",
    "\n",
    "printmd(\"### Encoder Output (Context Vector)\")\n",
    "print(f\"Input Text: '{input_text}'\")\n",
    "print(f\"Input Token Length: {input_ids.shape[1]}\")\n",
    "\n",
    "print(f\"\\nContext Vector Shape: {context_vector.shape}\")\n",
    "print(f\"  - Sequence Length (from input): {context_vector.shape[1]}\")\n",
    "print(f\"  - Hidden Dimension: {context_vector.shape[2]}\")\n",
    "print(f\"\\nFirst 5 Dimensions of the First Token's Vector:\")\n",
    "print(context_vector[0, 0, :5].numpy())\n",
    "\n",
    "printmd(\"\\n### Conceptual Decoder Usage\")\n",
    "printmd(\"This `context_vector` is provided to **every layer** of the Decoder. The Decoder uses this vector as the **Key (K)** and **Value (V)** in its cross-attention to condition the output on the entire input.\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The `context_vector` is the **single piece of information** transferred from the Encoder to the Decoder, containing the full, non-causal understanding of the source text.\")"
   ],
   "id": "b91cbf0192fc4eb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 10: Conceptual Comparison: T5 vs. GPT\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 10. üí° Conceptual Comparison: Encoder-Decoder (T5) vs. Decoder-Only (GPT) üÜö\")\n",
    "printmd(\"Summary of architectural differences and use cases.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "printmd(\"## üß± Architectural Differences\")\n",
    "\n",
    "printmd(\"| Feature | Encoder-Decoder (T5) | Decoder-Only (GPT) |\")\n",
    "printmd(\"|---|---|---|\")\n",
    "printmd(\"| **Architecture** | Two separate stacks (Encoder & Decoder) | Single stack (Decoder only) |\")\n",
    "printmd(\"| **Attention (Input)** | Bidirectional (Non-Causal) | Causal (Autoregressive) |\")\n",
    "printmd(\"| **Attention (Output)** | Causal + Cross-Attention | Causal (Self-Attention only) |\")\n",
    "printmd(\"| **Primary Tasks** | Seq2Seq: Translation, Summarization, Rewriting | Continuation: Generation, Prompt-based QA |\")\n",
    "printmd(\"| **Input/Output Length**| Output length can be different from input | Output is a continuation of the input |\")\n",
    "\n",
    "\n",
    "printmd(\"\\n## üéØ Task Suitability\")\n",
    "\n",
    "printmd(\"### Encoder-Decoder (e.g., T5, BART):\")\n",
    "printmd(\"**Best for:** Tasks where the output is a **transformation or condensation** of the input. They efficiently process the entire input first before starting generation.\")\n",
    "printmd(\"E.g.: Rewriting, Abstractive Summarization, Language Translation.\")\n",
    "\n",
    "printmd(\"### Decoder-Only (e.g., GPT):\")\n",
    "printmd(\"**Best for:** Tasks where the output is a **continuation** of the input. They excel at creative writing, chat, and prompt-based instruction following.\")\n",
    "printmd(\"E.g.: Chatbots, Creative Writing, Code Generation.\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The presence of the **Encoder** and the **Cross-Attention** mechanism is what distinguishes the Encoder-Decoder architecture, making it the preferred choice for sequence transformation tasks.\")"
   ],
   "id": "3b295c80ed12d808"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
