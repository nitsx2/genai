{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n"
   ],
   "id": "fa1e429773856e04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T22:00:47.463619Z",
     "start_time": "2025-11-01T22:00:39.996095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 1: Installation and Basic Text Generation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 1. Basic Text Generation - Understanding Autoregressive Behavior \")\n",
    "printmd(\"This is the core function of a decoder-only model: predicting the next token in a sequence.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "printmd(\"##  Installation (Run this cell once)\")\n",
    "printmd(\"```bash\\n!pip install transformers torch\\n```\")\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Set model configuration for cleaner generation output\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Input text\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Input tokens shape: {input_ids.shape}\")\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"GPT generates text **one token at a time**, using previously generated tokens as context. This is **autoregressive generation**the hallmark of decoder-only models.\")"
   ],
   "id": "63188436710d7448",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitin.sharma/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nitin.sharma/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 1. Basic Text Generation - Understanding Autoregressive Behavior "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "This is the core function of a decoder-only model: predicting the next token in a sequence."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "##  Installation (Run this cell once)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```bash\n!pip install transformers torch\n```"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The future of artificial intelligence is\n",
      "Input tokens shape: torch.Size([1, 6])\n",
      "\n",
      "Generated Text:\n",
      "The future of artificial intelligence is not in the hands of corporations, but in the hands of people, who are capable of creating the best possible world. This is something that we are not just talking about, but a fundamental change that is happening in our\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "GPT generates text **one token at a time**, using previously generated tokens as context. This is **autoregressive generation**the hallmark of decoder-only models."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T22:01:30.243472Z",
     "start_time": "2025-11-01T22:01:27.472784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 2: Step-by-Step Token Prediction\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded (assuming Notebook 1 was run or run these lines)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "printmd(\"# 2. Step-by-Step Token Prediction - Visualizing the Decoder Process И\")\n",
    "printmd(\"We manually step through the generation loop to see the model's top predictions at each step.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "prompt = \"Machine learning is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"Starting prompt: {prompt}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "current_ids = input_ids.clone()\n",
    "num_tokens_to_generate = 5\n",
    "for i in range(num_tokens_to_generate):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(current_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "    print(f\"\\nStep {i+1}:\")\n",
    "    print(f\"Current text: '{tokenizer.decode(current_ids[0])}'\")\n",
    "    print(f\"Top 5 predictions for next token:\")\n",
    "\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx]).replace('\\n', '\\\\n').replace(' ', '路')\n",
    "        print(f\"  '{token}' -> {prob.item():.4f}\")\n",
    "\n",
    "    # Use greedy decoding for a clear, consistent demonstration\n",
    "    next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "    current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
    "\n",
    "    selected_token = tokenizer.decode(next_token_id[0]).replace('\\n', '\\\\n').replace(' ', '路')\n",
    "    print(f\"Selected (Greedy): '{selected_token}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Final generated text: {tokenizer.decode(current_ids[0])}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"At each step, GPT looks at **ALL previous tokens** and predicts the next one. This sequential dependence confirms its decoder-only, autoregressive nature.\")"
   ],
   "id": "416f791a6d244c28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 2. Step-by-Step Token Prediction - Visualizing the Decoder Process И"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "We manually step through the generation loop to see the model's top predictions at each step."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Machine learning is\n",
      "\n",
      "============================================================\n",
      "\n",
      "Step 1:\n",
      "Current text: 'Machine learning is'\n",
      "Top 5 predictions for next token:\n",
      "  '路a' -> 0.1383\n",
      "  '路the' -> 0.0552\n",
      "  '路not' -> 0.0419\n",
      "  '路an' -> 0.0360\n",
      "  '路one' -> 0.0174\n",
      "Selected (Greedy): '路a'\n",
      "\n",
      "Step 2:\n",
      "Current text: 'Machine learning is a'\n",
      "Top 5 predictions for next token:\n",
      "  '路very' -> 0.0512\n",
      "  '路great' -> 0.0395\n",
      "  '路big' -> 0.0238\n",
      "  '路new' -> 0.0225\n",
      "  '路powerful' -> 0.0217\n",
      "Selected (Greedy): '路very'\n",
      "\n",
      "Step 3:\n",
      "Current text: 'Machine learning is a very'\n",
      "Top 5 predictions for next token:\n",
      "  '路powerful' -> 0.0764\n",
      "  '路important' -> 0.0678\n",
      "  '路complex' -> 0.0664\n",
      "  '路good' -> 0.0419\n",
      "  '路interesting' -> 0.0361\n",
      "Selected (Greedy): '路powerful'\n",
      "\n",
      "Step 4:\n",
      "Current text: 'Machine learning is a very powerful'\n",
      "Top 5 predictions for next token:\n",
      "  '路tool' -> 0.2763\n",
      "  '路and' -> 0.0863\n",
      "  '路way' -> 0.0759\n",
      "  '路technology' -> 0.0526\n",
      "  ',' -> 0.0329\n",
      "Selected (Greedy): '路tool'\n",
      "\n",
      "Step 5:\n",
      "Current text: 'Machine learning is a very powerful tool'\n",
      "Top 5 predictions for next token:\n",
      "  '路for' -> 0.1947\n",
      "  ',' -> 0.1575\n",
      "  '路that' -> 0.1230\n",
      "  '.' -> 0.1187\n",
      "  '路in' -> 0.0941\n",
      "Selected (Greedy): '路for'\n",
      "\n",
      "============================================================\n",
      "Final generated text: Machine learning is a very powerful tool for\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "At each step, GPT looks at **ALL previous tokens** and predicts the next one. This sequential dependence confirms its decoder-only, autoregressive nature."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T22:02:39.550397Z",
     "start_time": "2025-11-01T22:02:39.537778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 3: Understanding Masked Self-Attention\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 3. Understanding Masked Self-Attention (The Causal Mask) \")\n",
    "printmd(\"The masked attention mechanism is what defines the GPT decoder's inability to 'look ahead' at future tokens.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "sequence_length = 5\n",
    "# Create attention mask for decoder (causal/autoregressive mask)\n",
    "# This prevents positions from attending to future positions\n",
    "attention_mask = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "\n",
    "printmd(\"### Causal Attention Mask (Decoder-Only):\")\n",
    "printmd(\"`1 = can attend, 0 = cannot attend (masked)`\")\n",
    "print(attention_mask)\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"- Row i (Query token): can only attend to columns j <= i (Key tokens)\")\n",
    "print(\"- Row 0 (token 0): can only see token 0 (itself)\")\n",
    "print(\"- Row 4 (token 4): can see tokens 0, 1, 2, 3, 4\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The **triangular mask** ensures that when predicting token N, the model can only see tokens 0 to N-1. This left-to-right flow is crucial for autoregressive generation.\")"
   ],
   "id": "3459d85908722022",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 3. Understanding Masked Self-Attention (The Causal Mask) "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The masked attention mechanism is what defines the GPT decoder's inability to 'look ahead' at future tokens."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Causal Attention Mask (Decoder-Only):"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "`1 = can attend, 0 = cannot attend (masked)`"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Explanation:\n",
      "- Row i (Query token): can only attend to columns j <= i (Key tokens)\n",
      "- Row 0 (token 0): can only see token 0 (itself)\n",
      "- Row 4 (token 4): can see tokens 0, 1, 2, 3, 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The **triangular mask** ensures that when predicting token N, the model can only see tokens 0 to N-1. This left-to-right flow is crucial for autoregressive generation."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T23:30:17.275448Z",
     "start_time": "2025-11-01T23:30:14.905378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 4: Extracting and Analyzing Model Outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "printmd(\"# 4. Extracting and Analyzing Model Outputs \")\n",
    "printmd(\"We look inside the model to see the output shapes of the logits, hidden states, and attention weights.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "prompt = \"Python programming is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Get detailed model outputs including hidden states and attentions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "logits = outputs.logits                  # Predictions for next token\n",
    "hidden_states = outputs.hidden_states    # Hidden states from all layers\n",
    "attentions = outputs.attentions          # Attention weights from all layers\n",
    "\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"\\nModel has {len(hidden_states)} layers (including embedding)\")\n",
    "print(f\"GPT-2 Hidden Dimension (n_embd): {model.config.n_embd}\")\n",
    "\n",
    "printmd(\"### Logits Shape (Output Layer)\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
    "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
    "\n",
    "printmd(\"\\n### Hidden States Shape\")\n",
    "print(f\"Hidden state for last layer shape: {hidden_states[-1].shape}\")\n",
    "\n",
    "printmd(\"\\n### Attention Weights Shape\")\n",
    "print(f\"Attention weights for layer 0 shape: {attentions[0].shape}\")\n",
    "print(f\"  - Number of attention heads: {attentions[0].shape[1]}\")\n",
    "\n",
    "# Predict next token\n",
    "next_token_logits = logits[0, -1, :]\n",
    "next_token_id = torch.argmax(next_token_logits).item()\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "print(f\"\\nPredicted next token: '{next_token}'\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"GPT processes the entire input sequence through multiple decoder layers, with the final layer's output (logits) being converted to vocabulary probabilities.\")"
   ],
   "id": "939dc038406f739d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 4. Extracting and Analyzing Model Outputs "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "We look inside the model to see the output shapes of the logits, hidden states, and attention weights."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Python programming is\n",
      "Input shape: torch.Size([1, 3])\n",
      "\n",
      "Model has 13 layers (including embedding)\n",
      "GPT-2 Hidden Dimension (n_embd): 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Logits Shape (Output Layer)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 3, 50257])\n",
      "  - Sequence length: 3\n",
      "  - Vocabulary size: 50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n### Hidden States Shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state for last layer shape: torch.Size([1, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n### Attention Weights Shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 47\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHidden state for last layer shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhidden_states[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m printmd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m### Attention Weights Shape\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttention weights for layer 0 shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattentions[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  - Number of attention heads: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattentions[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Predict next token\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T23:32:15.119784Z",
     "start_time": "2025-11-01T23:32:09.814242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 5: Comparing Different Decoding Strategies\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "printmd(\"# 5. Comparing Different Decoding Strategies \")\n",
    "printmd(\"Decoder-only models can generate text in various ways by altering how the next token is sampled from the probability distribution.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 1. Greedy Decoding\n",
    "printmd(\"\\n**1. GREEDY DECODING (Deterministic):** Always picks the single highest probability token.\")\n",
    "output = model.generate(input_ids, max_length=30, do_sample=False)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# 2. Sampling with Temperature\n",
    "printmd(\"\\n**2. SAMPLING with temperature=0.7 (Randomness):** Introduces variance based on the probability distribution.\")\n",
    "output = model.generate(input_ids, max_length=30, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# 3. Top-k Sampling\n",
    "printmd(\"\\n**3. TOP-K SAMPLING (k=50):** Only samples from the top 50 most probable tokens.\")\n",
    "output = model.generate(input_ids, max_length=30, do_sample=True, top_k=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# 4. Top-p (Nucleus) Sampling\n",
    "printmd(\"\\n**4. TOP-P SAMPLING (p=0.9):** Samples from the smallest set of tokens whose cumulative probability exceeds 0.9.\")\n",
    "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.9)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# 5. Beam Search\n",
    "printmd(\"\\n**5. BEAM SEARCH (num_beams=5):** Keeps track of 5 best sequences at each step to find the globally optimal (most probable) output.\")\n",
    "output = model.generate(input_ids, max_length=30, num_beams=5, early_stopping=True)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"Decoder-only models support various decoding strategies, all based on the same principle: use the predicted token probabilities to select the next step in the sequence.\")"
   ],
   "id": "545612ea30542084",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 5. Comparing Different Decoding Strategies "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Decoder-only models can generate text in various ways by altering how the next token is sampled from the probability distribution."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**1. GREEDY DECODING (Deterministic):** Always picks the single highest probability token."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**2. SAMPLING with temperature=0.7 (Randomness):** Introduces variance based on the probability distribution."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the whole world would be an island of islands.\n",
      "\n",
      "In a time of the ocean, the oceans are a source of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**3. TOP-K SAMPLING (k=50):** Only samples from the top 50 most probable tokens."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was the king of the dwarves, a man of a great renown the most remarkable amongst all. Now, all this was that\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**4. TOP-P SAMPLING (p=0.9):** Samples from the smallest set of tokens whose cumulative probability exceeds 0.9."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was under assault by a mysterious force known as the Dark Ones.[28] The Dark Ones fought off the humans of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**5. BEAM SEARCH (num_beams=5):** Keeps track of 5 best sequences at each step to find the globally optimal (most probable) output."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it was said, there would be a time when the world would be a better place.\n",
      "\n",
      "It was a time when\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Decoder-only models support various decoding strategies, all based on the same principle: use the predicted token probabilities to select the next step in the sequence."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T23:43:55.531626Z",
     "start_time": "2025-11-01T23:43:52.616173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 6: Understanding Context Window\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "printmd(\"# 6. Understanding Context Window \")\n",
    "printmd(\"The context window defines the maximum number of tokens a model can attend to at any given time.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "print(f\"GPT-2 maximum context length: {max_length} tokens\\n\")\n",
    "\n",
    "# Create a long prompt\n",
    "long_text = \"AI \" * 600  # Creates a text over 1024 tokens\n",
    "input_ids = tokenizer.encode(long_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Long text token count: {input_ids.shape[1]} tokens\")\n",
    "\n",
    "if input_ids.shape[1] > max_length:\n",
    "    print(f\"Text exceeds max length! Truncating to last {max_length} tokens...\")\n",
    "    # Truncate the input to fit the context window\n",
    "    input_ids = input_ids[:, -max_length:]\n",
    "    print(f\"Truncated token count: {input_ids.shape[1]} tokens\")\n",
    "\n",
    "# Generate with truncated input\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=input_ids.shape[1] + 20)\n",
    "\n",
    "print(f\"\\nGenerated: ...{tokenizer.decode(output[0][-50:], skip_special_tokens=True)}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"Decoder-only models have a **fixed context window**. They can only attend to a limited number of previous tokens. Input exceeding this limit is simply ignored or truncated.\")"
   ],
   "id": "79a769f2b70ba969",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 6. Understanding Context Window "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The context window defines the maximum number of tokens a model can attend to at any given time."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 maximum context length: 1024 tokens\n",
      "\n",
      "Long text token count: 601 tokens\n",
      "\n",
      "Generated: ... AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Key Insight"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Decoder-only models have a **fixed context window**. They can only attend to a limited number of previous tokens. Input exceeding this limit is simply ignored or truncated."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 8: Using Different GPT Model Sizes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "printmd(\"# 8. Using Different GPT Model Sizes \")\n",
    "printmd(\"All GPT-2 variants share the same decoder-only architecture, differing only in the number of layers and hidden size.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "models_info = {\n",
    "    \"gpt2\": \"124M parameters\",\n",
    "    \"gpt2-medium\": \"355M parameters\",\n",
    "    \"gpt2-large\": \"774M parameters\",\n",
    "    \"gpt2-xl\": \"1.5B parameters\"\n",
    "}\n",
    "prompt = \"Artificial intelligence will\"\n",
    "printmd(\"Comparing GPT-2 model sizes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load and test the smallest model\n",
    "for model_name_check in [\"gpt2\"]:  # Use \"gpt2-medium\" or \"gpt2-large\" if you have enough RAM\n",
    "    print(f\"\\nModel: {model_name_check} ({models_info[model_name_check]})\")\n",
    "\n",
    "    tokenizer_temp = GPT2Tokenizer.from_pretrained(model_name_check)\n",
    "    model_temp = GPT2LMHeadModel.from_pretrained(model_name_check)\n",
    "    model_temp.eval()\n",
    "    tokenizer_temp.pad_token = tokenizer_temp.eos_token\n",
    "    model_temp.config.pad_token_id = model_temp.config.eos_token_id\n",
    "\n",
    "    input_ids = tokenizer_temp.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model_temp.generate(\n",
    "            input_ids,\n",
    "            max_length=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        generated = tokenizer_temp.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Generated: {generated}\")\n",
    "\n",
    "    print(f\"Config - Layers: {model_temp.config.n_layer}, \"\n",
    "          f\"Hidden size: {model_temp.config.n_embd}, \"\n",
    "          f\"Attention heads: {model_temp.config.n_head}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The core decoder-only architecture remains consistent across all sizes. **Larger models** gain capability by having more parameters (layers and hidden dimensions), not by changing their fundamental structure.\")"
   ],
   "id": "d83ef02301b8e620"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 9: Conditional Generation (Prompt Engineering)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "printmd(\"# 9. Conditional Generation - Showing Decoder Flexibility \")\n",
    "printmd(\"Decoder-only models can perform tasks traditionally reserved for encoder-decoder models by simply treating the entire input/output as a single sequence to be completed.\")\n",
    "printmd(\"---\")\n",
    "\n",
    "tasks = [\n",
    "    (\"Translation\", \"Translate English to French:\\nEnglish: Hello, how are you?\\nFrench:\"),\n",
    "    (\"Question Answering\", \"Q: What is the capital of France?\\nA:\"),\n",
    "    (\"Summarization\", \"Summarize this: Python is a high-level programming language known for its simplicity.\\nSummary:\"),\n",
    "    (\"Completion\", \"The three laws of robotics are\")]\n",
    "\n",
    "printmd(\"Decoder-only models can handle various tasks through prompting:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for task_name, prompt in tasks:\n",
    "    print(f\"\\n{task_name.upper()}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 30,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Output: {result}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"This demonstrates the **flexibility** of the decoder-only architecture. By structuring the prompt carefully (Prompt Engineering), you can condition the generation to perform specific tasks, despite the lack of a separate encoder.\")"
   ],
   "id": "44f5a17ba224132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# NOTEBOOK 10: Understanding Token Embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Ensure model and tokenizer are loaded\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "printmd(\"# 10. Understanding Token Embeddings \")\n",
    "printmd(\"The model starts by converting discrete tokens into dense vectors (embeddings).\")\n",
    "printmd(\"---\")\n",
    "\n",
    "# Access the token embedding layer\n",
    "embedding_layer = model.transformer.wte  # Word Token Embeddings\n",
    "\n",
    "tokens = [\"cat\", \"dog\", \"computer\", \"programming\"]\n",
    "printmd(\"### Token Embeddings:\")\n",
    "\n",
    "# Compute similarity between embeddings\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "embeddings = {}\n",
    "for token in tokens:\n",
    "    token_id = tokenizer.encode(token, add_special_tokens=False)[0]\n",
    "    embedding = embedding_layer.weight[token_id].detach()\n",
    "    embeddings[token] = embedding\n",
    "\n",
    "    print(f\"Token: '{token}' (ID: {token_id})\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"First 10 dimensions: {embedding[:10].numpy()}\\n\")\n",
    "\n",
    "printmd(\"### Embedding Similarities (Cosine Similarity):\")\n",
    "print(f\"cat <-> dog: {cosine_similarity(embeddings['cat'], embeddings['dog']):.4f}\")\n",
    "print(f\"cat <-> computer: {cosine_similarity(embeddings['cat'], embeddings['computer']):.4f}\")\n",
    "print(f\"dog <-> computer: {cosine_similarity(embeddings['dog'], embeddings['computer']):.4f}\")\n",
    "\n",
    "printmd(\"## Key Insight\")\n",
    "printmd(\"The initial embeddings provide the semantic foundation. Notice how the similarity between semantically related words ('cat' and 'dog') is often higher than unrelated words ('cat' and 'computer').\")"
   ],
   "id": "f61df196b07330d3"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
